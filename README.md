# Multimodal-Assistant

A multimodal assistant that understands both images and text, capable of answering questions about visual content in natural language. This project integrates image captioning and language generation models into a seamless pipeline with a real-time Gradio interface.

ğŸ” Features
ğŸ“¸ Image Understanding: Generate natural language captions from uploaded images using BLIP

ğŸ§¾ Question Answering: Answer user questions about the image context using a GPT-2 language model

ğŸ”€ Prompt Fusion: Combine image caption and user query into a single structured prompt for response generation

ğŸŒ Web UI: Gradio-based interface for real-time interaction (image upload + question input)

âš™ï¸ Optional support for switching to larger LLMs like Mistral, Falcon, or LLaMA

ğŸš€ Demo
Run locally:

bash
Sao chÃ©p
Chá»‰nh sá»­a
git clone https://github.com/yourusername/multimodal-ai-assistant.git
cd multimodal-ai-assistant
pip install -r requirements.txt
python app.py
Then open your browser: http://localhost:7860

ğŸ§© Tech Stack
Language: Python

Models: BLIP (image captioning), GPT-2 (language generation)

Libraries: Hugging Face Transformers, PyTorch, Gradio

Deployment: Local Gradio app (can be extended to Hugging Face Spaces)

ğŸ“ Folder Structure
bash
Sao chÃ©p
Chá»‰nh sá»­a
multimodal-ai-assistant/
â”‚
â”œâ”€â”€ app.py              # Gradio UI
â”œâ”€â”€ main.py             # Core pipeline logic
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ /src/
â”‚   â”œâ”€â”€ vision.py       # Image captioning logic (BLIP)
â”‚   â””â”€â”€ llm.py          # Language model response (GPT-2)
â””â”€â”€ /data/              # Sample images (optional)
ğŸ“Œ Future Improvements
 Support Vietnamese language (prompt translation or Vietnamese LLM)

 Integrate BLIP-2 for VQA

 Add object detection for localized queries

 Save user interaction history

ğŸ™‹ Author
Nguyá»…n Máº¡nh DÅ©ng
